{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 本地CPU环境测试，ONNX和TensorRT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 安装依赖包"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting opencv-python\n",
            "  Downloading opencv_python-4.10.0.84-cp37-abi3-macosx_12_0_x86_64.whl (56.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 56.5 MB 566 kB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /Users/hepeng01/Library/Python/3.9/lib/python/site-packages (from opencv-python) (1.26.2)\n",
            "Installing collected packages: opencv-python\n",
            "Successfully installed opencv-python-4.10.0.84\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 2.2.2\n",
            "Uninstalling torch-2.2.2:\n",
            "  Successfully uninstalled torch-2.2.2\n",
            "Found existing installation: torchvision 0.17.2\n",
            "Uninstalling torchvision-0.17.2:\n",
            "  Successfully uninstalled torchvision-0.17.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip uninstall -y torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting torch==1.10.0\n",
            "  Downloading torch-1.10.0-cp39-none-macosx_10_9_x86_64.whl (147.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 147.1 MB 643 kB/s eta 0:00:01█████████                  | 64.2 MB 600 kB/s eta 0:02:19\n",
            "\u001b[?25hCollecting torchvision==0.11.1\n",
            "  Downloading torchvision-0.11.1-cp39-cp39-macosx_10_9_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 3.6 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /Users/hepeng01/Library/Python/3.9/lib/python/site-packages (from torch==1.10.0) (4.9.0)\n",
            "Requirement already satisfied: numpy in /Users/hepeng01/Library/Python/3.9/lib/python/site-packages (from torchvision==0.11.1) (1.26.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /Users/hepeng01/Library/Python/3.9/lib/python/site-packages (from torchvision==0.11.1) (10.2.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "Successfully installed torch-1.10.0 torchvision-0.11.1\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install torch==1.10.0 torchvision==0.11.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.16.2-cp39-cp39-macosx_11_0_universal2.whl (16.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.5 MB 2.9 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /Users/hepeng01/Library/Python/3.9/lib/python/site-packages (from onnx) (1.26.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /Users/hepeng01/Library/Python/3.9/lib/python/site-packages (from onnx) (4.25.1)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.16.2\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3smIT_TnE4Z",
        "outputId": "bd5bde3f-037b-43de-d638-9c4f4fbf3269"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.18.1-cp39-cp39-macosx_11_0_universal2.whl (15.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.9 MB 453 kB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: packaging in /Users/hepeng01/Library/Python/3.9/lib/python/site-packages (from onnxruntime) (23.2)\n",
            "Requirement already satisfied: protobuf in /Users/hepeng01/Library/Python/3.9/lib/python/site-packages (from onnxruntime) (4.25.1)\n",
            "Collecting coloredlogs\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 1.1 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: sympy in /Users/hepeng01/Library/Python/3.9/lib/python/site-packages (from onnxruntime) (1.12)\n",
            "Collecting flatbuffers\n",
            "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.21.6 in /Users/hepeng01/Library/Python/3.9/lib/python/site-packages (from onnxruntime) (1.26.2)\n",
            "Collecting humanfriendly>=9.1\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 2.9 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /Users/hepeng01/Library/Python/3.9/lib/python/site-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Installing collected packages: humanfriendly, flatbuffers, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 flatbuffers-24.3.25 humanfriendly-10.0 onnxruntime-1.18.1\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/hepeng01/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "#忽略警告\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'net' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# cpu = True #是否使用cpu\u001b[39;00m\n\u001b[1;32m     28\u001b[0m cpu \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;66;03m#当前环境没有gpu使用cpu\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(\u001b[43mnet\u001b[49m, trained_weigth_path, cpu)\n\u001b[1;32m     30\u001b[0m model\u001b[38;5;241m.\u001b[39meval() \u001b[38;5;66;03m#设置模型为评估模式\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
          ]
        }
      ],
      "source": [
        "# 加载预训练模型\n",
        "def load_model(model, pretrained_path, load_to_cpu):\n",
        "    print('Loading pretrained model from {}'.format(pretrained_path))\n",
        "    if load_to_cpu:\n",
        "        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage)\n",
        "    else:\n",
        "        device = torch.cuda.current_device()\n",
        "        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage.cuda(device))\n",
        "\n",
        "    if \"state_dict\" in pretrained_dict.keys():\n",
        "        pretrained_dict = remove_prefix(pretrained_dict['state_dict'], 'module.')\n",
        "    else:\n",
        "        pretrained_dict = remove_prefix(pretrained_dict, 'module.')\n",
        "\n",
        "    # 加载权重\n",
        "    model.load_state_dict(pretrained_dict, strict=False)\n",
        "    return model\n",
        "\n",
        "def remove_prefix(state_dict, prefix):\n",
        "    print('remove prefix \\'{}\\''.format(prefix))\n",
        "    f = lambda x: x.split(prefix, 1)[-1] if x.startswith(prefix) else x\n",
        "    return {f(key): value for key, value in state_dict.items()}\n",
        "\n",
        "#初始化模型\n",
        "\n",
        "# 使用backbone网络\n",
        "network = 'resnet50'  #'mobile0.25'\n",
        "\n",
        "# 加载模型配置\n",
        "cfg = cfg_re50 if network == \"resnet50\" else cfg_mnet\n",
        "print(cfg)\n",
        "\n",
        "# 初始化模型\n",
        "net = RetinaFace(cfg=cfg, phase='test')\n",
        "print(net)\n",
        "\n",
        "#预训练模型参数\n",
        "trained_weigth_path = './Resnet50_Final.pth' #mobilenet0.25_epoch_10.pth\n",
        "\n",
        "# cpu = True #是否使用cpu\n",
        "cpu = not torch.cuda.is_available() #当前环境没有gpu使用cpu\n",
        "model = load_model(net, trained_weigth_path, cpu)\n",
        "model.eval() #设置模型为评估模式"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from itertools import product as product\n",
        "import numpy as np\n",
        "from math import ceil\n",
        "\n",
        "#生成先验框\n",
        "class PriorBox(object):\n",
        "    def __init__(self, cfg, image_size=None, phase='train'):\n",
        "        super(PriorBox, self).__init__()\n",
        "        self.min_sizes = cfg['min_sizes']\n",
        "        self.steps = cfg['steps']\n",
        "        self.clip = cfg['clip']\n",
        "        self.image_size = image_size\n",
        "        self.feature_maps = [[ceil(self.image_size[0]/step), ceil(self.image_size[1]/step)] for step in self.steps]\n",
        "        self.name = \"s\"\n",
        "\n",
        "    def forward(self):\n",
        "        anchors = []\n",
        "        for k, f in enumerate(self.feature_maps):\n",
        "            min_sizes = self.min_sizes[k]\n",
        "            for i, j in product(range(f[0]), range(f[1])):\n",
        "                for min_size in min_sizes:\n",
        "                    s_kx = min_size / self.image_size[1]\n",
        "                    s_ky = min_size / self.image_size[0]\n",
        "                    dense_cx = [x * self.steps[k] / self.image_size[1] for x in [j + 0.5]]\n",
        "                    dense_cy = [y * self.steps[k] / self.image_size[0] for y in [i + 0.5]]\n",
        "                    for cy, cx in product(dense_cy, dense_cx):\n",
        "                        anchors += [cx, cy, s_kx, s_ky]\n",
        "\n",
        "        # back to torch land\n",
        "        output = torch.Tensor(anchors).view(-1, 4)\n",
        "        if self.clip:\n",
        "            output.clamp_(max=1, min=0)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "#解码先验框\n",
        "def py_cpu_nms(dets, thresh):\n",
        "    \"\"\"Pure Python NMS baseline.\"\"\"\n",
        "    x1 = dets[:, 0]\n",
        "    y1 = dets[:, 1]\n",
        "    x2 = dets[:, 2]\n",
        "    y2 = dets[:, 3]\n",
        "    scores = dets[:, 4]\n",
        "\n",
        "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
        "    order = scores.argsort()[::-1]\n",
        "\n",
        "    keep = []\n",
        "    while order.size > 0:\n",
        "        i = order[0]\n",
        "        keep.append(i)\n",
        "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
        "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
        "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
        "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
        "\n",
        "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
        "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
        "        inter = w * h\n",
        "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
        "\n",
        "        inds = np.where(ovr <= thresh)[0]\n",
        "        order = order[inds + 1]\n",
        "\n",
        "    return keep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "#解码位置\n",
        "def decode_loc(loc, priors, variances):\n",
        "    \"\"\"Decode locations from predictions using priors to undo\n",
        "    the encoding we did for offset regression at train time.\n",
        "    Args:\n",
        "        loc (tensor): location predictions for loc layers,\n",
        "            Shape: [num_priors,4]\n",
        "        priors (tensor): Prior boxes in center-offset form.\n",
        "            Shape: [num_priors,4].\n",
        "        variances: (list[float]) Variances of priorboxes\n",
        "    Return:\n",
        "        decoded bounding box predictions\n",
        "    \"\"\"\n",
        "\n",
        "    boxes = torch.cat((\n",
        "        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n",
        "        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n",
        "    boxes[:, :2] -= boxes[:, 2:] / 2\n",
        "    boxes[:, 2:] += boxes[:, :2]\n",
        "    return boxes\n",
        "#解码关键点\n",
        "def decode_landm(pre, priors, variances):\n",
        "    \"\"\"Decode landm from predictions using priors to undo\n",
        "    the encoding we did for offset regression at train time.\n",
        "    Args:\n",
        "        pre (tensor): landm predictions for loc layers,\n",
        "            Shape: [num_priors,10]\n",
        "        priors (tensor): Prior boxes in center-offset form.\n",
        "            Shape: [num_priors,4].\n",
        "        variances: (list[float]) Variances of priorboxes\n",
        "    Return:\n",
        "        decoded landm predictions\n",
        "    \"\"\"\n",
        "    landms = torch.cat((priors[:, :2] + pre[:, :2] * variances[0] * priors[:, 2:],\n",
        "                        priors[:, :2] + pre[:, 2:4] * variances[0] * priors[:, 2:],\n",
        "                        priors[:, :2] + pre[:, 4:6] * variances[0] * priors[:, 2:],\n",
        "                        priors[:, :2] + pre[:, 6:8] * variances[0] * priors[:, 2:],\n",
        "                        priors[:, :2] + pre[:, 8:10] * variances[0] * priors[:, 2:],\n",
        "                        ), dim=1)\n",
        "    return landms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'name': 'Resnet50', 'min_sizes': [[16, 32], [64, 128], [256, 512]], 'steps': [8, 16, 32], 'variance': [0.1, 0.2], 'clip': False, 'loc_weight': 2.0, 'gpu_train': False, 'batch_size': 24, 'ngpu': 0, 'epoch': 100, 'decay1': 70, 'decay2': 90, 'image_size': 840, 'pretrain': True, 'return_layers': {'layer2': 1, 'layer3': 2, 'layer4': 3}, 'in_channel': 256, 'out_channel': 256}\n"
          ]
        }
      ],
      "source": [
        "cfg_mnet = {\n",
        "    'name': 'mobilenet0.25',\n",
        "    'min_sizes': [[16, 32], [64, 128], [256, 512]],\n",
        "    'steps': [8, 16, 32],\n",
        "    'variance': [0.1, 0.2],\n",
        "    'clip': False,\n",
        "    'loc_weight': 2.0,\n",
        "    'gpu_train': True,\n",
        "    'batch_size': 32,\n",
        "    'ngpu': 1,\n",
        "    'epoch': 250,\n",
        "    'decay1': 190,\n",
        "    'decay2': 220,\n",
        "    'image_size': 640,\n",
        "    'pretrain': True,\n",
        "    'return_layers': {'stage1': 1, 'stage2': 2, 'stage3': 3},\n",
        "    'in_channel': 32,\n",
        "    'out_channel': 64\n",
        "}\n",
        "\n",
        "cfg_re50 = {\n",
        "    'name': 'Resnet50',\n",
        "    'min_sizes': [[16, 32], [64, 128], [256, 512]],\n",
        "    'steps': [8, 16, 32],\n",
        "    'variance': [0.1, 0.2],\n",
        "    'clip': False,\n",
        "    'loc_weight': 2.0,\n",
        "    'gpu_train': False,\n",
        "    'batch_size': 24,\n",
        "    'ngpu': 0,\n",
        "    'epoch': 100,\n",
        "    'decay1': 70,\n",
        "    'decay2': 90,\n",
        "    'image_size': 840,\n",
        "    'pretrain': True,\n",
        "    'return_layers': {'layer2': 1, 'layer3': 2, 'layer4': 3},\n",
        "    'in_channel': 256,\n",
        "    'out_channel': 256\n",
        "}\n",
        "\n",
        "# 使用backbone网络\n",
        "network = 'resnet50'  #'mobile0.25'\n",
        "# 加载模型配置\n",
        "cfg = cfg_re50 if network == \"resnet50\" else cfg_mnet\n",
        "print(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4zdhELF3rfbc"
      },
      "outputs": [],
      "source": [
        "# 加载图片并预处理\n",
        "def preprocess_image(image_path):\n",
        "  img_raw = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
        "  if img_raw is None:\n",
        "    print('image cant open, check please')\n",
        "    return\n",
        "\n",
        "  img = np.float32(img_raw) #转为浮点数\n",
        "  im_height, im_width, _ = img.shape #获取高，宽，忽略第三个参数通道数\n",
        "\n",
        "  scale = torch.Tensor([img.shape[1], img.shape[0], img.shape[1], img.shape[0]])\n",
        "\n",
        "  img -= (104, 117, 123) #减去均值进行归一化，RGB 固定104, 117, 123\n",
        "  img = img.transpose(2, 0, 1) #转置图像维度 维度从 (height, width, channels) 转换为 (channels, height, width)\n",
        "  img = np.expand_dims(img, axis=0) #添加批量维度 形状从 (channels, height, width) 变为 (1, channels, height, width)。\n",
        "\n",
        "  return img, img_raw, scale\n",
        "\n",
        "\n",
        "# 解码预测结果 \n",
        "def decode_results(loc, conf, landms, resize, scale, img_shape):\n",
        "  im_height = img_shape[2]\n",
        "  im_width = img_shape[3] \n",
        "\n",
        "  #### 解码预测结果 #####\n",
        "  priorbox = PriorBox(cfg, image_size=(im_height, im_width)) #生成先验框（prior boxes），用于帮助模型预测目标的位置\n",
        "  priors = priorbox.forward() #生成先验框\n",
        "  prior_data = priors.data #将先验框数据转换为 NumPy 数组 \n",
        "\n",
        "  #解码位置预测，将模型的输出解码为实际的边框坐标 \n",
        "  # boxes = decode(loc.data.squeeze(0), prior_data, cfg['variance']) # （修改点） 使用先验框和模型的输出（loc[0]）解码出实际的边框坐标 cfg['variance'] 是解码过程中使用的方差。\n",
        "  boxes = decode_loc(torch.tensor(loc[0]), torch.tensor(prior_data), cfg['variance']) #使用先验框和模型的输出（loc[0]）解码出实际的边框坐标。cfg['variance'] 是解码过程中使用的方差。\n",
        "  boxes = boxes * scale / resize #对解码后的边框进行缩放，使其与原图的大小一致。scale 是之前定义的缩放比例张量，resize 是缩放因子，这里为1。\n",
        "  boxes = boxes.cpu().numpy() #将解码后的边框转换为 NumPy 数组\n",
        "\n",
        "  #提取置信度 \n",
        "  # scores = conf.squeeze(0).data.cpu().numpy()[:, 1] #提取了每个预测框的置信度分数 （修改点）\n",
        "  scores = conf[0][:, 1] #提取了每个预测框的置信度分数\n",
        "\n",
        "  #解码关键点预测\n",
        "  # landms = decode_landm(landms.data.squeeze(0), prior_data, cfg['variance']) （修改点）\n",
        "  landms = decode_landm(torch.tensor(landms[0]), torch.tensor(prior_data), cfg['variance'])\n",
        "\n",
        "  scale1 = torch.Tensor([img_shape[3], img_shape[2], img_shape[3], img_shape[2],\n",
        "                        img_shape[3], img_shape[2], img_shape[3], img_shape[2],\n",
        "                        img_shape[3], img_shape[2]])\n",
        "  landms = landms * scale1 / resize\n",
        "  landms = landms.cpu().numpy()\n",
        "\n",
        "  return boxes, scores, landms\n",
        "\n",
        "# 过滤检测框\n",
        "def filter_boxes(boxes, scores, landms, confidence_threshold, nms_threshold):\n",
        "  # 过滤低置信度检测结果\n",
        "  inds = np.where(scores > confidence_threshold)[0]\n",
        "  boxes = boxes[inds] #保留置信度高于阈值的边框\n",
        "  landms = landms[inds] #保留置信度高于阈值的关键点\n",
        "  scores = scores[inds] #保留置信度高于阈值的置信度分数\n",
        "\n",
        "  # 按置信度排序\n",
        "  order = scores.argsort()[::-1] #[::-1] 表示从高到低排序。\n",
        "  boxes = boxes[order] #按排序后的索引重新排列边框\n",
        "  landms = landms[order] #按排序后的索引重新排列关键点\n",
        "  scores = scores[order] #按排序后的索引重新排列置信度分数\n",
        "\n",
        "  # 将边框和置信度分数水平堆叠在一起，形成一个新的数组 dets。scores[:, np.newaxis] 将置信度分数转换为列向量。\n",
        "  dets = np.hstack((boxes, scores[:, np.newaxis], landms)).astype(np.float32, copy=False)\n",
        "\n",
        "  # Apply NMS\n",
        "  # 使用 NMS 算法去除重叠的检测框。nms_threshold 是 NMS 的阈值，py_cpu_nms 是一个实现 NMS 的函数。keep 是保留下来的检测框的索引。\n",
        "  keep = py_cpu_nms(dets[:, :5], nms_threshold)\n",
        "  dets = dets[keep, :] #保留 NMS 后的检测框\n",
        "  landms = landms[keep] #保留 NMS 后的关键点\n",
        "\n",
        "  return dets\n",
        "\n",
        "# 在图上绘制检测结果\n",
        "def draw_detection(image, dets, save_path, vis_thres=0.6):\n",
        "  if dets is None:\n",
        "     return\n",
        "  # 绘制检测结果（可能多张人脸）\n",
        "  for i, b in enumerate(dets):\n",
        "      if b[4] < vis_thres:\n",
        "          continue\n",
        "      text = \"{:.4f}\".format(b[4])\n",
        "      b = list(map(int, b))\n",
        "      cv2.rectangle(image, (b[0], b[1]), (b[2], b[3]), (0, 0, 255), 2)\n",
        "      \n",
        "      cx = b[0]\n",
        "      cy = b[1] + 12\n",
        "      cv2.putText(image, text, (cx, cy),\n",
        "                  cv2.FONT_HERSHEY_DUPLEX, 0.5, (255, 255, 255))\n",
        "\n",
        "      # 绘制关键点\n",
        "      cv2.circle(image, (b[5], b[6]), 1, (0, 0, 255), 4)\n",
        "      cv2.circle(image, (b[7], b[8]), 1, (0, 255, 255), 4)\n",
        "      cv2.circle(image, (b[9], b[10]), 1, (255, 0, 255), 4)\n",
        "      cv2.circle(image, (b[11], b[12]), 1, (0, 255, 0), 4)\n",
        "      cv2.circle(image, (b[13], b[14]), 1, (255, 0, 0), 4)\n",
        "      \n",
        "      print(f\"Box {i}: {b}\")\n",
        "  # 保存结果图像\n",
        "  cv2.imwrite(save_path, image)\n",
        "  print(f\"Detection result saved to {save_path}\")\n",
        "\n",
        "  # 展示结果图像\n",
        "  # from google.colab.patches import cv2_imshow\n",
        "  # cv2_imshow(img_raw)\n",
        "\n",
        "# 图片保存路径\n",
        "def get_save_path(image_path, suffix=\"_new\"):\n",
        "  directory, filename = os.path.split(image_path)\n",
        "  name, ext = os.path.splitext(filename)\n",
        "  save_path = os.path.join(directory, f\"{name}{suffix}{ext}\")\n",
        "  return save_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 使用ONNXRuntime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loc shape: (1, 16952, 4)\n",
            "conf shape: (1, 16952, 2)\n",
            "landms shape: (1, 16952, 10)\n",
            "Box 0: [275, 0, 469, 266, 0, 288, 92, 366, 97, 297, 148, 292, 199, 347, 204]\n",
            "Detection result saved to test/1_onnx.png\n"
          ]
        }
      ],
      "source": [
        "import onnxruntime as ort\n",
        "# 加载 ONNX 模型\n",
        "onnx_model_path = \"model_repository/kavin_face_model/1/model.onnx\"\n",
        "ort_session = ort.InferenceSession(onnx_model_path)\n",
        "\n",
        "# 执行ONNX模型推理\n",
        "def inference_onnx(ort_session, img):\n",
        "  # 前向传播\n",
        "  input_name = ort_session.get_inputs()[0].name #获取输入名称\n",
        "  loc, conf, landms = ort_session.run(None, {input_name: img}) #使用 ONNX Runtime 进行模型推理，第一个参数 None 表示我们要获取模型的所有输出，第二个参数是一个字典，键是输入名称，值是输入数据。\n",
        "\n",
        "  # loc：位置预测（bounding box location predictions）\n",
        "  # conf：置信度（confidence scores）\n",
        "  # landms：关键点（landmark predictions）\n",
        "  print(\"loc shape:\", loc.shape)\n",
        "  print(\"conf shape:\", conf.shape)\n",
        "  print(\"landms shape:\", landms.shape)  \n",
        "  return loc, conf, landms\n",
        "\n",
        "#人脸检测,使用ONNX模型\n",
        "def check_face_use_onnx(ort_session, image_path):\n",
        "  confidence_threshold = 0.5 # 置信度阈值，过滤低置信度检测结果\n",
        "  nms_threshold = 0.4 # NMS 阈值，过滤重叠的检测框\n",
        "  vis_thres = 0.6 # 可视化阈值，只绘制置信度高于该阈值的检测结果\n",
        "  resize = 1 # 使用原图\n",
        "\n",
        "  # 加载图片并预处理\n",
        "  img, img_raw, scale = preprocess_image(image_path)\n",
        "\n",
        "  # 执行ONNX模型推理\n",
        "  loc, conf, landms = inference_onnx(ort_session, img)\n",
        "  \n",
        "  # 推理结果解码\n",
        "  #img.shape 表示模型输入的图像的形状。(1, 3, 308, 450)  \n",
        "  boxes, scores, landms = decode_results(loc, conf, landms, resize, scale, img.shape)\n",
        "\n",
        "  # 过滤检测框\n",
        "  dets = filter_boxes(boxes, scores, landms, confidence_threshold, nms_threshold)\n",
        "\n",
        "  # 在图像上结果标注并保存\n",
        "  save_path = get_save_path(image_path, \"_onnx\") \n",
        "  draw_detection(img_raw, dets, save_path, vis_thres)\n",
        "\n",
        "# 检测人脸\n",
        "check_face_use_onnx(ort_session, \"test/1.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 使用Docker API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loc shape: (1, 68432, 4)\n",
            "conf shape: (1, 68432, 2)\n",
            "landms shape: (1, 68432, 10)\n",
            "Box 0: [1458, 240, 1518, 320, 0, 1475, 271, 1502, 271, 1488, 287, 1475, 297, 1500, 296]\n",
            "Box 1: [700, 225, 758, 305, 0, 715, 257, 742, 257, 729, 274, 716, 283, 741, 282]\n",
            "Box 2: [1073, 232, 1132, 308, 0, 1089, 266, 1116, 264, 1104, 282, 1092, 291, 1116, 289]\n",
            "Box 3: [360, 242, 417, 320, 0, 384, 273, 409, 275, 400, 292, 383, 302, 401, 303]\n",
            "Detection result saved to test/2_api.jpg\n"
          ]
        }
      ],
      "source": [
        "# 使用接口推理\n",
        "import requests\n",
        "def inference_api(img):\n",
        "  # 构造推理请求的JSON结构\n",
        "  infer_request = {\n",
        "        \"inputs\": [\n",
        "            {\n",
        "                \"name\": \"input\",\n",
        "                \"shape\": img.shape,\n",
        "                \"datatype\": \"FP32\",\n",
        "                \"data\": img.flatten().tolist()\n",
        "            }\n",
        "        ]\n",
        "  }  \n",
        "  # 发送推理请求\n",
        "  try:\n",
        "    url = 'http://localhost:8000/v2/models/kavin_face_model/infer'\n",
        "    response = requests.post(url, json=infer_request)\n",
        "    result = response.json()\n",
        "    if response.status_code != 200:\n",
        "      raise Exception(result)\n",
        "  except Exception as e:\n",
        "    print(\"Error in inference request:\", e)\n",
        "    return\n",
        "\n",
        "  # 解析推理结果\n",
        "  conf = np.array(result['outputs'][0]['data']).reshape(result['outputs'][0]['shape'])\n",
        "  landms = np.array(result['outputs'][1]['data']).reshape(result['outputs'][1]['shape'])\n",
        "  loc = np.array(result['outputs'][2]['data']).reshape(result['outputs'][2]['shape'])\n",
        "\n",
        "  # 打印输出的形状\n",
        "  print(\"loc shape:\", loc.shape)\n",
        "  print(\"conf shape:\", conf.shape)\n",
        "  print(\"landms shape:\", landms.shape)\n",
        "  return loc, conf, landms\n",
        "\n",
        "#人脸检测,使用API\n",
        "def check_face_use_api(image_path):\n",
        "  confidence_threshold = 0.5\n",
        "  nms_threshold = 0.4\n",
        "  vis_thres = 0.6\n",
        "  resize = 1 # 使用原图\n",
        "\n",
        "  # 加载图片并预处理\n",
        "  img, img_raw, scale = preprocess_image(image_path)\n",
        "\n",
        "  # 执行API推理\n",
        "  loc, conf, landms = inference_api(img) \n",
        "\n",
        "  # 推理结果解码\n",
        "  #img.shape 表示模型输入的图像的形状。(1, 3, 308, 450)  \n",
        "  boxes, scores, landms = decode_results(loc, conf, landms, resize, scale, img.shape)\n",
        "\n",
        "  # 过滤检测框\n",
        "  dets = filter_boxes(boxes, scores, landms, confidence_threshold, nms_threshold)\n",
        "\n",
        "  # 在图像上结果标注并保存\n",
        "  save_path = get_save_path(image_path, \"_api\") \n",
        "  draw_detection(img_raw, dets, save_path, vis_thres)\n",
        "\n",
        "check_face_use_api(\"test/2.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZxJ_1JfBgJg"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
